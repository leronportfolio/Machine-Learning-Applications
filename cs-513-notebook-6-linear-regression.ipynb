{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7700cc0",
   "metadata": {
    "papermill": {
     "duration": 0.012117,
     "end_time": "2023-05-16T19:00:09.121733",
     "exception": false,
     "start_time": "2023-05-16T19:00:09.109616",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook contains two parts. **Part 1, Multiple Linear Regression**, provides you an opportunity to demonstrate your ability to apply course concepts by implementing a training function for multiple linear regression. **Part 2, California Housing Prices**, provides you an opportunity to practice using widely-used ML libraries and an ML workflow to solve a regression problem.\n",
    "\n",
    "**You do not need to complete Part 1 in order to complete Part 2**. If you get stuck on Part 1, and choose to work on Part 2, be sure that all of your code for Part 1 runs without error. You can comment out your code in Part 1 if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cca1a63",
   "metadata": {
    "papermill": {
     "duration": 0.011886,
     "end_time": "2023-05-16T19:00:09.146047",
     "exception": false,
     "start_time": "2023-05-16T19:00:09.134161",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Part 1: Implementing Multiple Linear Regression\n",
    "\n",
    "Given a simple MultipleLinearRegressor, and a simple training set of housing data, demonstrate your ability to implement a multiple linear regression model's `fit` function, such that it properly trains its linear model using gradient descent.\n",
    "\n",
    "## The UnivariateLinearRegressor\n",
    "\n",
    "Let's first review the UnivariateLinearRegressor, which you should find familiar, and you do not need to modify. Notice that the `fit` method uses a fixed number of iterations, only for simplicity and experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c059a309",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T19:00:09.171275Z",
     "iopub.status.busy": "2023-05-16T19:00:09.170849Z",
     "iopub.status.idle": "2023-05-16T19:00:09.188402Z",
     "shell.execute_reply": "2023-05-16T19:00:09.187261Z"
    },
    "papermill": {
     "duration": 0.032843,
     "end_time": "2023-05-16T19:00:09.190727",
     "exception": false,
     "start_time": "2023-05-16T19:00:09.157884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UnivariateLinearRegressor:\n",
    "\n",
    "    def __init__(self, w = 0, b = 0, alpha = 0.1):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        for _ in range(0, 10):\n",
    "            delta_w = self.alpha * self._d_cost_function_w(x_train, y_train)\n",
    "            delta_b = self.alpha * self._d_cost_function_b(x_train, y_train)\n",
    "            self.w = self.w - delta_w\n",
    "            self.b = self.b - delta_b\n",
    "\n",
    "    def _d_cost_function_w(self, x_train, y_train):\n",
    "        sum = 0\n",
    "        for i in range(len(x_train)):\n",
    "            sum += (self.predict(x_train[i]) - y_train[i]) * x_train[i]\n",
    "        return sum / len(x_train)\n",
    "\n",
    "    def _d_cost_function_b(self, x_train, y_train):\n",
    "        sum = 0\n",
    "        for i in range(len(x_train)):\n",
    "            sum += (self.predict(x_train[i]) - y_train[i])\n",
    "        return sum / len(x_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.w * x + self.b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ccaaaf",
   "metadata": {
    "papermill": {
     "duration": 0.011382,
     "end_time": "2023-05-16T19:00:09.215216",
     "exception": false,
     "start_time": "2023-05-16T19:00:09.203834",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next, consider the following simple training examples, which you should also find familiar, that represent the square feet and prices of houses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b46f2bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T19:00:09.241233Z",
     "iopub.status.busy": "2023-05-16T19:00:09.240765Z",
     "iopub.status.idle": "2023-05-16T19:00:09.246468Z",
     "shell.execute_reply": "2023-05-16T19:00:09.245241Z"
    },
    "papermill": {
     "duration": 0.021394,
     "end_time": "2023-05-16T19:00:09.248846",
     "exception": false,
     "start_time": "2023-05-16T19:00:09.227452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train = [1.0, 2.0]\n",
    "y_train = [300.0, 500.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab6b77b",
   "metadata": {
    "papermill": {
     "duration": 0.014223,
     "end_time": "2023-05-16T19:00:09.277933",
     "exception": false,
     "start_time": "2023-05-16T19:00:09.263710",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As demonstrated in the related Exploration, we can instantiate, train and make predictions with our UnivariateLinearRegressor as follows. Notice how we first instantiate our UnivariateLinearRegressor with a _single_ weight, and the bias and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f19fe6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T19:00:09.310592Z",
     "iopub.status.busy": "2023-05-16T19:00:09.309687Z",
     "iopub.status.idle": "2023-05-16T19:00:09.320790Z",
     "shell.execute_reply": "2023-05-16T19:00:09.319252Z"
    },
    "papermill": {
     "duration": 0.032187,
     "end_time": "2023-05-16T19:00:09.323744",
     "exception": false,
     "start_time": "2023-05-16T19:00:09.291557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The price of a 1,000 sqft house is 301.4502082643262\n",
      "The price of a 2,000 sqft house is 488.7867275295899\n",
      "The price of an 8,000 sqft house is 1612.805843121172\n"
     ]
    }
   ],
   "source": [
    "regressor = UnivariateLinearRegressor(0, 0, 0.1)\n",
    "regressor.fit(x_train, y_train)\n",
    "\n",
    "small_house_price = regressor.predict(1.0)\n",
    "print(f\"The price of a 1,000 sqft house is {small_house_price}\")\n",
    "\n",
    "medium_house_price = regressor.predict(2.0)\n",
    "print(f\"The price of a 2,000 sqft house is {medium_house_price}\")\n",
    "\n",
    "big_house_price = regressor.predict(8.0)\n",
    "print(f\"The price of an 8,000 sqft house is {big_house_price}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415c215d",
   "metadata": {
    "papermill": {
     "duration": 0.01265,
     "end_time": "2023-05-16T19:00:09.352712",
     "exception": false,
     "start_time": "2023-05-16T19:00:09.340062",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Observing the results, we can see that the model has made its way toward converging on its line of best fit. However, we are intentionally limiting the amount of training in `fit`, and therefore truncating the training. Again, we are limiting this only for simplicity and experimentation. Try increasing the steps of gradient descent to 500 and re-run the code cells, and notice that the predictions become more accurate.\n",
    "\n",
    "This concludes a review of our UnivariateLinearRegressor. Notice that this implementation intentionally handles only one dimension of input. In the example above, this one dimension is the size in square feet of a house.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48279377",
   "metadata": {
    "papermill": {
     "duration": 0.01244,
     "end_time": "2023-05-16T19:00:09.377384",
     "exception": false,
     "start_time": "2023-05-16T19:00:09.364944",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## The MultipleLinearRegressor\n",
    "\n",
    "While our simple UnivariateLinearRegressor works well for just a single dimension of input, we would like to make predictions based on multiple features, such as square feet, number of bedrooms, the number of floors, and the age of a house.\n",
    "\n",
    "To demonstrate your understanding of features, vectors and gradient descent, try completing the implementation of a MultipleLinearRegressor. We begin with the implementation below, which has a complete `predict` method and method stubs for `fit` and the partial derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d8322dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T19:00:09.404051Z",
     "iopub.status.busy": "2023-05-16T19:00:09.403667Z",
     "iopub.status.idle": "2023-05-16T19:00:09.419766Z",
     "shell.execute_reply": "2023-05-16T19:00:09.418589Z"
    },
    "papermill": {
     "duration": 0.032333,
     "end_time": "2023-05-16T19:00:09.422318",
     "exception": false,
     "start_time": "2023-05-16T19:00:09.389985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultipleLinearRegressor:\n",
    "    \n",
    "    def __init__(self, w=[], b=0, learning_rate=0.01):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.learning_rate = learning_rate\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "    \n",
    "    def fit(self, x_train, y_train):\n",
    "        x_train = np.array(x_train)\n",
    "        self.mean = np.mean(x_train, axis=0)\n",
    "        self.std = np.std(x_train, axis=0)\n",
    "        x_train_norm = (x_train - self.mean) / self.std\n",
    "        self.w = np.zeros(x_train_norm.shape[1])\n",
    "        \n",
    "        for _ in range(0, 10000):\n",
    "            delta_w = self.learning_rate * self._d_cost_function_w(x_train_norm, y_train)\n",
    "            delta_b = self.learning_rate * self._d_cost_function_b(x_train_norm, y_train)\n",
    "            self.w = np.where(np.isnan(self.w), 0, self.w)\n",
    "            self.b = np.where(np.isnan(self.b), 0, self.b)\n",
    "            self.w = np.where(np.isinf(self.w), 0, self.w)\n",
    "            self.b = np.where(np.isinf(self.b), 0, self.b)\n",
    "            self.w = self.w - delta_w\n",
    "            self.b = self.b - delta_b\n",
    "    \n",
    "    def _d_cost_function_w(self, x_train, y_train):\n",
    "        y_pred = np.dot(x_train, self.w) + self.b\n",
    "        error = y_pred - y_train\n",
    "        gradient = []\n",
    "        for i in range(x_train.shape[1]):\n",
    "            gradient_i = 0\n",
    "            for j in range(len(x_train)):\n",
    "                gradient_i += error[j] * x_train[j][i]\n",
    "            gradient.append(gradient_i / len(x_train))\n",
    "        return np.array(gradient)\n",
    "    \n",
    "    def _d_cost_function_b(self, x_train, y_train):\n",
    "        y_pred = np.dot(x_train, self.w) + self.b\n",
    "        error = y_pred - y_train\n",
    "        return np.sum(error) / len(x_train)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x_norm = (x - self.mean) / self.std\n",
    "        return self._dot_product(self.w, x_norm) + self.b\n",
    "    \n",
    "    def _dot_product(self, a, b):\n",
    "        return sum(pair[0] * pair[1] for pair in zip(a, b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6722cd37",
   "metadata": {
    "papermill": {
     "duration": 0.011864,
     "end_time": "2023-05-16T19:00:09.446547",
     "exception": false,
     "start_time": "2023-05-16T19:00:09.434683",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As we shall see in a moment, your goal will be to implement `fit` and `_d_cost_function_w` and `_d_cost_function_b`. For now, let's take a look at the training set and see how our current implementation behaves.\n",
    "\n",
    "We'll start with a simple contrived data set with four examples, already split for you. Each training example in `x_train` represents the size, number of bedrooms, number of floors and the age of a house. Each value in `y_train` represents the price of the house in thousands of dollars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33f73cb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T19:00:09.472100Z",
     "iopub.status.busy": "2023-05-16T19:00:09.471685Z",
     "iopub.status.idle": "2023-05-16T19:00:09.477710Z",
     "shell.execute_reply": "2023-05-16T19:00:09.476574Z"
    },
    "papermill": {
     "duration": 0.021524,
     "end_time": "2023-05-16T19:00:09.480052",
     "exception": false,
     "start_time": "2023-05-16T19:00:09.458528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train = [\n",
    "    [2104.0, 5.0, 1.0, 45.0],\n",
    "    [1416.0, 3.0, 2.0, 40.0],\n",
    "    [1534.0, 3.0, 2.0, 30.0],\n",
    "    [852.0, 2.0, 1.0, 36.0]\n",
    "]\n",
    "y_train = [460.0, 232.0, 315.0, 178.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c67e435",
   "metadata": {
    "papermill": {
     "duration": 0.012105,
     "end_time": "2023-05-16T19:00:09.504547",
     "exception": false,
     "start_time": "2023-05-16T19:00:09.492442",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Notice that `x_train` now contains vectors representing the features of each house, and each vector contains four features. Since we know that our linear regression model will need one weight for each feature, we should instantiate it with a _vector_ of weights, along with a bias and our learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7328ded8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T19:00:09.531710Z",
     "iopub.status.busy": "2023-05-16T19:00:09.531296Z",
     "iopub.status.idle": "2023-05-16T19:00:10.361321Z",
     "shell.execute_reply": "2023-05-16T19:00:10.360092Z"
    },
    "papermill": {
     "duration": 0.846777,
     "end_time": "2023-05-16T19:00:10.364124",
     "exception": false,
     "start_time": "2023-05-16T19:00:09.517347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "regressor = MultipleLinearRegressor([0, 0, 0, 0], 0, 0.1)\n",
    "regressor.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae95479",
   "metadata": {
    "papermill": {
     "duration": 0.011535,
     "end_time": "2023-05-16T19:00:10.387705",
     "exception": false,
     "start_time": "2023-05-16T19:00:10.376170",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Even though our implementation is incomplete, we can try to make some predictions. Notice that, to make a prediction, we should provide the `predict` method with a vector of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0e0dee0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T19:00:10.414405Z",
     "iopub.status.busy": "2023-05-16T19:00:10.414015Z",
     "iopub.status.idle": "2023-05-16T19:00:10.422421Z",
     "shell.execute_reply": "2023-05-16T19:00:10.421537Z"
    },
    "papermill": {
     "duration": 0.024706,
     "end_time": "2023-05-16T19:00:10.425037",
     "exception": false,
     "start_time": "2023-05-16T19:00:10.400331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The actual price of a 2,104 sqft house with 5 bedrooms, 1 floor, that is 45-years old is 460 thousand dollars\n",
      "The predicted price of a 2,104 sqft house with 5 bedrooms, 1 floor, that is 45-years old is 459.9999999999997 thousand dollars\n",
      "The actual price of a 1,416 sqft house with 3 bedrooms, 2 floors, that is 40 years old is 232 thousand dollars\n",
      "The predicted price of a 1,416 sqft house with 3 bedrooms, 2 floors, that is 40 years old is 231.99999999999991 thousand dollars\n",
      "The actual price of a 1,534 sqft house with 3 bedrooms, 2 floors, that is 30 years old is 315 thousand dollars\n",
      "The predicated price of a 1,534 sqft house with 3 bedrooms, 2 floors, that is 30 years old is 314.99999999999966 thousand dollars\n",
      "The actual price of an 852 sqft house with 2 bedrooms, 1 floor, that is 36 years old is 178 thousand dollars\n",
      "The predicted price of this house is 177.99999999999983\n"
     ]
    }
   ],
   "source": [
    "# 'Test Run' Code Cell, Referred to in \"What to Do\" #2.\n",
    "\n",
    "first_house_price = regressor.predict([2104.0, 5.0, 1.0, 45.0])\n",
    "print(f\"The actual price of a 2,104 sqft house with 5 bedrooms, 1 floor, that is 45-years old is 460 thousand dollars\")\n",
    "print(f\"The predicted price of a 2,104 sqft house with 5 bedrooms, 1 floor, that is 45-years old is {first_house_price} thousand dollars\")\n",
    "\n",
    "second_house_price = regressor.predict([1416.0, 3.0, 2.0, 40.0])\n",
    "print(f\"The actual price of a 1,416 sqft house with 3 bedrooms, 2 floors, that is 40 years old is 232 thousand dollars\")\n",
    "print(f\"The predicted price of a 1,416 sqft house with 3 bedrooms, 2 floors, that is 40 years old is {second_house_price} thousand dollars\")\n",
    "\n",
    "third_house_price = regressor.predict([1534.0, 3.0, 2.0, 30.0])\n",
    "print(f\"The actual price of a 1,534 sqft house with 3 bedrooms, 2 floors, that is 30 years old is 315 thousand dollars\")\n",
    "print(f\"The predicated price of a 1,534 sqft house with 3 bedrooms, 2 floors, that is 30 years old is {third_house_price} thousand dollars\")\n",
    "\n",
    "small_house_price = regressor.predict([852.0, 2.0, 1.0, 36.0])\n",
    "print(f\"The actual price of an 852 sqft house with 2 bedrooms, 1 floor, that is 36 years old is 178 thousand dollars\")\n",
    "print(f\"The predicted price of this house is {small_house_price}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba1df91",
   "metadata": {
    "papermill": {
     "duration": 0.011725,
     "end_time": "2023-05-16T19:00:10.449037",
     "exception": false,
     "start_time": "2023-05-16T19:00:10.437312",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Notice how, for each example, our MultipleLinearRegressor model is predicting a 0.\n",
    "\n",
    "Our goal is to complete the implementation of MultipleLinearRegressor, ensuring that we can properly train it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63725e4c",
   "metadata": {
    "papermill": {
     "duration": 0.011652,
     "end_time": "2023-05-16T19:00:10.472661",
     "exception": false,
     "start_time": "2023-05-16T19:00:10.461009",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## What to Do\n",
    "\n",
    "Implement `fit`, `_d_cost_function_w` and `_d_cost_function_b`, to represent an appropriate gradient descent algorithm that trains our multiple linear regression model. When complete, you should see the model produce price predictions that begin to approach a \"best fit\" for the simple training data above (note: there are particular reasons why the fit will not be as 'perfect' as our univariate example). Here are some suggestions for completing your implementation.\n",
    "\n",
    "1. Modify the existing MultipleLinearRegressor class definition above.\n",
    "2. Run your code frequently, using _Run All_ and running the code in the \"Test Run\" code cell above.\n",
    "2. Draw inspiration from the UnivariateLinearRegressor - the structure of gradient descent remains the same, we just need to handle a vector of weights and features.\n",
    "3. Consider replicating the small steps taken in the exploration. Start with `fit`.\n",
    "4. Review the Exploration content and familiarize yourself with the expressions for computing the partial derivatives with respect to `w` and `b` when using a _vector_ of weights and features.\n",
    "5. Implement just _one_ of the partial derivative functions first, and verify that the prediction output has changed.\n",
    "6. For convenience, you can create a new code cell with the class definition, data, instantiation and usage all in one code cell if you wish. But when complete, please be sure that you remove it, and that the MultipleLinearRegressor class definition above is complete.\n",
    "\n",
    "The best tip for thinking about this challenge is to become intimately familiar with the expressions for computing the gradients, or partial derivatives, for w and b. Then, try first working out on paper how your implementation of these computations might work, given the vector of weights and features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d928274",
   "metadata": {
    "papermill": {
     "duration": 0.011755,
     "end_time": "2023-05-16T19:00:10.496343",
     "exception": false,
     "start_time": "2023-05-16T19:00:10.484588",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ðŸ’¡ Conclusion\n",
    "\n",
    "First, I reviewed the existing code for MultipleLinearRegressor and noted that it required the implementation of fit, _d_cost_function_w, and _d_cost_function_b to perform the appropriate gradient descent algorithm to train the multiple linear regression model. I then use similar formatting as the LinearRegressor section and the expressions for computing the partial derivatives win terms of w and b when using a vector of weights and features.I implemented the fit function and computed the delta_w and delta_b for each iteration using the alpha value provided and updated the self.w and self.b values accordingly. I converted the input x_train to a numpy array and computed the mean and standard deviation along the appropriate axis. After that, I then used the features by subtracting the mean and dividng it by the std. I initialized the weight vector as vector of zeros after. Inside the training loop, I made it calculate cost function with respect to the weight vector and bias terms using partial derivatives, and updated them by subtracting respective gradients that is scaled by the learning rate. I tried repeating this process multiple times until it satisfy what I am looking for. In the end, the MultipleLinearRegression can take on many multiple features, and calculate gradients for each of the weight and bias (and update them) in order to help better the relationship between different inputs (multiple) features and target variable.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1262fcce",
   "metadata": {
    "papermill": {
     "duration": 0.011621,
     "end_time": "2023-05-16T19:00:10.520058",
     "exception": false,
     "start_time": "2023-05-16T19:00:10.508437",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Part 2: Predicting California Housing Prices\n",
    "\n",
    "_Attribution: Special thanks to Dr. Roi Yehoshua_\n",
    "\n",
    "In this, the second, part of this notebook, you will construct a guided experiment to analyze the quality of a linear regression model for predicting real housing prices. We'll use a version of the [california housing data](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html) by Kelley and Barry. Take a moment now to [familiarize yourself with the version of this data set provded by sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html), and you can take a look at [a version of this data on Kaggle](https://www.kaggle.com/datasets/camnugent/california-housing-prices). (Note that, the version on Kaggle has an extra column, ocean_proximity, which you should ignore.)\n",
    "\n",
    "As you progress through this notebook, complete each code cell, run them, and complete the Knowledge Checks.\n",
    "\n",
    "We'll begin by loading the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf6e27c",
   "metadata": {
    "papermill": {
     "duration": 0.011502,
     "end_time": "2023-05-16T19:00:10.543574",
     "exception": false,
     "start_time": "2023-05-16T19:00:10.532072",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 1: Loading the Data Set\n",
    "\n",
    "For convenience, we shall rely on the \"california housing set\" provided by scikit-learn. We'll first import a few typical libraries, and fetch the data set.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "np.random.seed(0)\n",
    "data = fetch_california_housing(as_frame = True)\n",
    "print(data.DESCR)\n",
    "```\n",
    "\n",
    "Try doing the same in a code cell here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73e29ac0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T19:00:10.569990Z",
     "iopub.status.busy": "2023-05-16T19:00:10.569606Z",
     "iopub.status.idle": "2023-05-16T19:00:15.356113Z",
     "shell.execute_reply": "2023-05-16T19:00:15.355330Z"
    },
    "papermill": {
     "duration": 4.803003,
     "end_time": "2023-05-16T19:00:15.358751",
     "exception": false,
     "start_time": "2023-05-16T19:00:10.555748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 20640\n",
      "\n",
      "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      "    :Attribute Information:\n",
      "        - MedInc        median income in block group\n",
      "        - HouseAge      median house age in block group\n",
      "        - AveRooms      average number of rooms per household\n",
      "        - AveBedrms     average number of bedrooms per household\n",
      "        - Population    block group population\n",
      "        - AveOccup      average number of household members\n",
      "        - Latitude      block group latitude\n",
      "        - Longitude     block group longitude\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
      "\n",
      "The target variable is the median house value for California districts,\n",
      "expressed in hundreds of thousands of dollars ($100,000).\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "A household is a group of people residing within a home. Since the average\n",
      "number of rooms and bedrooms in this dataset are provided per household, these\n",
      "columns may take surprisingly large values for block groups with few households\n",
      "and many empty houses, such as vacation resorts.\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "      Statistics and Probability Letters, 33 (1997) 291-297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "np.random.seed(0)\n",
    "data = fetch_california_housing(as_frame = True)\n",
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fbaae4",
   "metadata": {
    "papermill": {
     "duration": 0.011864,
     "end_time": "2023-05-16T19:00:15.383000",
     "exception": false,
     "start_time": "2023-05-16T19:00:15.371136",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### ðŸ’¡ Knowledge Check 1\n",
    "\n",
    "Demonstrate your understanding of the general characteristics of the data set by summarizing it here. (What is this data set, and what does it contain? What are the attributes, what are their types, and what do they mean? What is the target value? Is there missing data? Etc.)\n",
    "\n",
    "The California Housing dataset contains information about the housing in California. It includes various features of the houses, location, number of rooms, population, and median income of the surrounding area. Based on the data set, it has a total of 20,640 records, and each record has 8 attributes. \n",
    "\n",
    "The attributes are:\n",
    "        - MedInc        median income in block group\n",
    "        - HouseAge      median house age in block group\n",
    "        - AveRooms      average number of rooms per household\n",
    "        - AveBedrms     average number of bedrooms per household\n",
    "        - Population    block group population\n",
    "        - AveOccup      average number of household members\n",
    "        - Latitude      block group latitude\n",
    "        - Longitude     block group longitude\n",
    "        \n",
    "The target variable is expressed in hundreds of thousands of dollars for median house value. It looks like all of attributes are numeric, except for latitude and longitude. The target variable for Median House Value is the variable that I want to predict. \n",
    "\n",
    "Based on my observations the data set contains no missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f714b0cd",
   "metadata": {
    "papermill": {
     "duration": 0.011742,
     "end_time": "2023-05-16T19:00:15.407144",
     "exception": false,
     "start_time": "2023-05-16T19:00:15.395402",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now that our data set is loaded, let's explore what we have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c280a0aa",
   "metadata": {
    "papermill": {
     "duration": 0.011714,
     "end_time": "2023-05-16T19:00:15.430997",
     "exception": false,
     "start_time": "2023-05-16T19:00:15.419283",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 2: Exploring the Data Set\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ad02de",
   "metadata": {
    "papermill": {
     "duration": 0.011843,
     "end_time": "2023-05-16T19:00:15.455060",
     "exception": false,
     "start_time": "2023-05-16T19:00:15.443217",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's quickly investigate some examples in the data set. Since `data` is a sklearn Bunch object, we can obtain the pandas DataFrame and investigate its shape, to determine the number of rows and columns, and to inspect the first few rows of data.\n",
    "\n",
    "```python\n",
    "print(data.frame.shape)\n",
    "data.frame.head()\n",
    "```\n",
    "\n",
    "Go ahead and investigate the first few rows of the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5984903",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T19:00:15.481617Z",
     "iopub.status.busy": "2023-05-16T19:00:15.480553Z",
     "iopub.status.idle": "2023-05-16T19:00:15.514032Z",
     "shell.execute_reply": "2023-05-16T19:00:15.513217Z"
    },
    "papermill": {
     "duration": 0.048972,
     "end_time": "2023-05-16T19:00:15.516313",
     "exception": false,
     "start_time": "2023-05-16T19:00:15.467341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedHouseVal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  MedHouseVal  \n",
       "0    -122.23        4.526  \n",
       "1    -122.22        3.585  \n",
       "2    -122.24        3.521  \n",
       "3    -122.25        3.413  \n",
       "4    -122.25        3.422  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.frame.shape)\n",
    "data.frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3479086",
   "metadata": {
    "papermill": {
     "duration": 0.012527,
     "end_time": "2023-05-16T19:00:15.541627",
     "exception": false,
     "start_time": "2023-05-16T19:00:15.529100",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### ðŸ’¡ Knowledge Check 2\n",
    "\n",
    "What do the `shape` and `head` reveal about this data set?\n",
    "\n",
    "The shape of the dataset shows that it has 20640 rows and 9 columns.\n",
    "\n",
    "The head method shows the first 5 rows of the dataset along with the header, whichs shows a short summary when looking into the type of data that is contained in each column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461b92e6",
   "metadata": {
    "papermill": {
     "duration": 0.012266,
     "end_time": "2023-05-16T19:00:15.566510",
     "exception": false,
     "start_time": "2023-05-16T19:00:15.554244",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 3: Preparing Training and Test Sets\n",
    "\n",
    "To train and test our linear regression model, we will need to split our data set. We'll use the `data` and `target` attributes of the Bunch to retrieve the feature set and target prediction values. Then, we'll reach for the handy `train_test_split` method from sklearn.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "housing_attributes, prices = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing_attributes, prices, test_size = 0.2)\n",
    "X_train.head()\n",
    "```\n",
    "\n",
    "Go ahead and split the data set into training and test sets here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fb554c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T19:00:15.593414Z",
     "iopub.status.busy": "2023-05-16T19:00:15.592968Z",
     "iopub.status.idle": "2023-05-16T19:00:15.709333Z",
     "shell.execute_reply": "2023-05-16T19:00:15.708236Z"
    },
    "papermill": {
     "duration": 0.132436,
     "end_time": "2023-05-16T19:00:15.711738",
     "exception": false,
     "start_time": "2023-05-16T19:00:15.579302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12069</th>\n",
       "      <td>4.2386</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.723077</td>\n",
       "      <td>1.169231</td>\n",
       "      <td>228.0</td>\n",
       "      <td>3.507692</td>\n",
       "      <td>33.83</td>\n",
       "      <td>-117.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15925</th>\n",
       "      <td>4.3898</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.326622</td>\n",
       "      <td>1.100671</td>\n",
       "      <td>1485.0</td>\n",
       "      <td>3.322148</td>\n",
       "      <td>37.73</td>\n",
       "      <td>-122.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11162</th>\n",
       "      <td>3.9333</td>\n",
       "      <td>26.0</td>\n",
       "      <td>4.668478</td>\n",
       "      <td>1.046196</td>\n",
       "      <td>1022.0</td>\n",
       "      <td>2.777174</td>\n",
       "      <td>33.83</td>\n",
       "      <td>-118.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4904</th>\n",
       "      <td>1.4653</td>\n",
       "      <td>38.0</td>\n",
       "      <td>3.383495</td>\n",
       "      <td>1.009709</td>\n",
       "      <td>749.0</td>\n",
       "      <td>3.635922</td>\n",
       "      <td>34.01</td>\n",
       "      <td>-118.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4683</th>\n",
       "      <td>3.1765</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.119792</td>\n",
       "      <td>1.043403</td>\n",
       "      <td>1135.0</td>\n",
       "      <td>1.970486</td>\n",
       "      <td>34.08</td>\n",
       "      <td>-118.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "12069  4.2386       6.0  7.723077   1.169231       228.0  3.507692     33.83   \n",
       "15925  4.3898      52.0  5.326622   1.100671      1485.0  3.322148     37.73   \n",
       "11162  3.9333      26.0  4.668478   1.046196      1022.0  2.777174     33.83   \n",
       "4904   1.4653      38.0  3.383495   1.009709       749.0  3.635922     34.01   \n",
       "4683   3.1765      52.0  4.119792   1.043403      1135.0  1.970486     34.08   \n",
       "\n",
       "       Longitude  \n",
       "12069    -117.55  \n",
       "15925    -122.44  \n",
       "11162    -118.00  \n",
       "4904     -118.26  \n",
       "4683     -118.36  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "housing_attributes, prices = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing_attributes, prices, test_size = 0.2)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f834e0dd",
   "metadata": {
    "papermill": {
     "duration": 0.012957,
     "end_time": "2023-05-16T19:00:15.737617",
     "exception": false,
     "start_time": "2023-05-16T19:00:15.724660",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### ðŸ’¡ Knowledge Check 3\n",
    "\n",
    "Approximately how many examples are in the training and test sets?\n",
    "\n",
    "The are 16512 training set examples, and the test set is 4128 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7914a66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T19:00:15.765978Z",
     "iopub.status.busy": "2023-05-16T19:00:15.765232Z",
     "iopub.status.idle": "2023-05-16T19:00:15.771233Z",
     "shell.execute_reply": "2023-05-16T19:00:15.770072Z"
    },
    "papermill": {
     "duration": 0.022862,
     "end_time": "2023-05-16T19:00:15.773915",
     "exception": false,
     "start_time": "2023-05-16T19:00:15.751053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 16512\n",
      "Test set size: 4128\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"Test set size:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1333f70",
   "metadata": {
    "papermill": {
     "duration": 0.012585,
     "end_time": "2023-05-16T19:00:15.799601",
     "exception": false,
     "start_time": "2023-05-16T19:00:15.787016",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 4: Pre-Processing and Training\n",
    "\n",
    "Before applying our regression model, we would like to standardize the training set. To do this, we'll use the sklearn StandardScaler. Once we standardize the data, we will use it to train a linear regression model. In our case, we will experiment with the scikit-learn [SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html), a linear regression model that trains via stochastic gradient descent (SGD). Please be sure to take a look at [the documentation for SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html).\n",
    "\n",
    "To demonstrate a new feature in scikit-learn, and to give you some new ideas in your own future work, we will illustrate a small \"machine learning pipeline,\" using the scikit-learn [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) class.\n",
    "\n",
    "A Pipeline is handy for \"setting up\" multiple pre-processing steps that will run one after the other. The Pipeline can also end in a training step with a model. This enables us to provide the Pipeline our training data, and with one method call, complete both pre-processing and training in one step.\n",
    "\n",
    "We'll import the necessary libraries, create our Pipeline, fill it with a StandardScalar and SGDRegressor, and run the Pipeline.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', SGDRegressor())\n",
    "])\n",
    "```\n",
    "\n",
    "Try importing the necessary libraries and building your Pipeline below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8df06450",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T19:00:15.827135Z",
     "iopub.status.busy": "2023-05-16T19:00:15.826422Z",
     "iopub.status.idle": "2023-05-16T19:00:15.933108Z",
     "shell.execute_reply": "2023-05-16T19:00:15.932057Z"
    },
    "papermill": {
     "duration": 0.123349,
     "end_time": "2023-05-16T19:00:15.935835",
     "exception": false,
     "start_time": "2023-05-16T19:00:15.812486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', SGDRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9651370",
   "metadata": {
    "papermill": {
     "duration": 0.012637,
     "end_time": "2023-05-16T19:00:15.961560",
     "exception": false,
     "start_time": "2023-05-16T19:00:15.948923",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "With our Pipeline created, we can now invoke the Pipeline's `fit` method, passing it the training data. Behind the scenes, the Pipeline will standardize our training data, and also invoke our SGDRegressor's `fit` method with the transformed training data.\n",
    "\n",
    "```\n",
    "pipeline.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "Try kicking off the Pipeline below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e02fefdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T19:00:15.989844Z",
     "iopub.status.busy": "2023-05-16T19:00:15.989037Z",
     "iopub.status.idle": "2023-05-16T19:00:16.030037Z",
     "shell.execute_reply": "2023-05-16T19:00:16.028817Z"
    },
    "papermill": {
     "duration": 0.057795,
     "end_time": "2023-05-16T19:00:16.032490",
     "exception": false,
     "start_time": "2023-05-16T19:00:15.974695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()), (&#x27;regressor&#x27;, SGDRegressor())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()), (&#x27;regressor&#x27;, SGDRegressor())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDRegressor</label><div class=\"sk-toggleable__content\"><pre>SGDRegressor()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()), ('regressor', SGDRegressor())])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc2b361",
   "metadata": {
    "papermill": {
     "duration": 0.012962,
     "end_time": "2023-05-16T19:00:16.058898",
     "exception": false,
     "start_time": "2023-05-16T19:00:16.045936",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "With our model now trained, let us analyze the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc09e43",
   "metadata": {
    "papermill": {
     "duration": 0.013373,
     "end_time": "2023-05-16T19:00:16.086744",
     "exception": false,
     "start_time": "2023-05-16T19:00:16.073371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### ðŸ’¡ Knowledge Check 4\n",
    "\n",
    "Investigate the parameters passed to the Pipeline initializer. Notice our use of the strings `'scaler'` and `'regressor'`. What purpose do these serve, and are we required to use those specific strings, or can we \"make up\" our own meaningful names for each component of the Pipeline?\n",
    "\n",
    "In the Pipeline initializer, the strings scaler and regression are used as names or keys for each component of the Pipeline. These keys can help identify each component in the Pipeline. We can choose to use any other string as long as it makes sense and not reused as a name for another component in the Pipeline. An example can be: instead of scaler, we could use data_scaling. And instead of regressor, we could use linear_model that helps us identify the linear regression model component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab4207b",
   "metadata": {
    "papermill": {
     "duration": 0.012727,
     "end_time": "2023-05-16T19:00:16.112954",
     "exception": false,
     "start_time": "2023-05-16T19:00:16.100227",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 5: Model Validation\n",
    "\n",
    "We have conducted an initial round of training using a data set that may or may not have strong linear tendencies, and we have employed a basic, unconfigured SGDRegressor model to see what baseline quality we can achieve. Let's investigate the \"coefficient of determination,\" R^2, via the model's `score` method. We will invoke this `score` method via the Pipeline, since it has ownership of our SGDRegressor model. We would love to see a value as close to 1.0 as possible.\n",
    "\n",
    "We can generate an R^2 score with both the training data and the test data to validate the quality of our model.\n",
    "\n",
    "```python\n",
    "training_score = pipeline.score(X_train, y_train)\n",
    "print(f\"Training score: {training_score:.6f}\")\n",
    "\n",
    "test_score = pipeline.score(X_test, y_test)\n",
    "print(f\"Test score: {test_score:.6f}\")\n",
    "```\n",
    "\n",
    "Go ahead and generate and print the score based on the training data, and the score based on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81c6e83f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T19:00:16.141688Z",
     "iopub.status.busy": "2023-05-16T19:00:16.141288Z",
     "iopub.status.idle": "2023-05-16T19:00:16.160016Z",
     "shell.execute_reply": "2023-05-16T19:00:16.158396Z"
    },
    "papermill": {
     "duration": 0.038768,
     "end_time": "2023-05-16T19:00:16.164877",
     "exception": false,
     "start_time": "2023-05-16T19:00:16.126109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: -154.744672\n",
      "Test score: -1413.434270\n"
     ]
    }
   ],
   "source": [
    "training_score = pipeline.score(X_train, y_train)\n",
    "print(f\"Training score: {training_score:.6f}\")\n",
    "\n",
    "test_score = pipeline.score(X_test, y_test)\n",
    "print(f\"Test score: {test_score:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc3fdd1",
   "metadata": {
    "papermill": {
     "duration": 0.027677,
     "end_time": "2023-05-16T19:00:16.220930",
     "exception": false,
     "start_time": "2023-05-16T19:00:16.193253",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### ðŸ’¡ Knowledge Check 5\n",
    "\n",
    "What are the scores for the training and test sets? What do they indicate? Are they good? How do you know? (Hint: Have you read the documentation for the score method of SGDRegressor?)\n",
    "\n",
    "The training score is -154.744672 and the test score is -1413.434270. It seems that the model isn't doing too well with predictions and is considered not accurate when performing comapre to the baseline model. It may be that it is not capturing underlying patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e48705",
   "metadata": {
    "papermill": {
     "duration": 0.026143,
     "end_time": "2023-05-16T19:00:16.274968",
     "exception": false,
     "start_time": "2023-05-16T19:00:16.248825",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step 6: Adjusting the Model (Experiment)\n",
    "\n",
    "If we spend time reviewing the documentation of SGDRegressor, we find that the default instantiation uses particular default hyperparameters. Now it's your turn. Based on the concepts in the course and your understanding of linear regression, how might you \"tune\" the SGDRegressor instance in the Pipeline?\n",
    "\n",
    "Try setting up a new Pipeline as an experiment, and try passing different parameter configurations to SGDRegressor's initializer, and investigate the results. You might set up your experiment like the following. Notice how we have specified a `penalty` of `None` as a demonstrated experiment.\n",
    "\n",
    "```python\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', SGDRegressor(penalty = None))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "training_score = pipeline.score(X_train, y_train)\n",
    "print(f\"Training score: {training_score:.6f}\")\n",
    "\n",
    "test_score = pipeline.score(X_test, y_test)\n",
    "print(f\"Test score: {test_score:.6f}\")\n",
    "```\n",
    "\n",
    "Create a similar experiment here, and try a few different initialization parameters for SGDRegressor. How might you increase its performance score? (Think about the important concepts of a linear regression model that uses gradient descent. Be sure to try customizing the most important hyperparameters.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59b6e302",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T19:00:16.304049Z",
     "iopub.status.busy": "2023-05-16T19:00:16.303622Z",
     "iopub.status.idle": "2023-05-16T19:00:16.341508Z",
     "shell.execute_reply": "2023-05-16T19:00:16.340046Z"
    },
    "papermill": {
     "duration": 0.056927,
     "end_time": "2023-05-16T19:00:16.345609",
     "exception": false,
     "start_time": "2023-05-16T19:00:16.288682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: -2355925001336889344.000000\n",
      "Test score: -6412475811969347584.000000\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', SGDRegressor(learning_rate='constant', eta0=0.01, alpha=0.001, max_iter=1000))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "training_score = pipeline.score(X_train, y_train)\n",
    "print(f\"Training score: {training_score:.6f}\")\n",
    "\n",
    "test_score = pipeline.score(X_test, y_test)\n",
    "print(f\"Test score: {test_score:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994e00af",
   "metadata": {
    "papermill": {
     "duration": 0.027278,
     "end_time": "2023-05-16T19:00:16.401179",
     "exception": false,
     "start_time": "2023-05-16T19:00:16.373901",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "See if you can make any improvement to the training and test scores. Then, see if your models can achieve a score between 0.0 and 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c92c34",
   "metadata": {
    "papermill": {
     "duration": 0.027664,
     "end_time": "2023-05-16T19:00:16.457189",
     "exception": false,
     "start_time": "2023-05-16T19:00:16.429525",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### ðŸ’¡ Knowledge Check 6\n",
    "\n",
    "Based on the concepts in the Explorations regarding linear regression and gradient descent, what is perhaps the single most important hyperparameter for a linear regression model? What SGDRegressor initialization parameter lets you specify the value for this important hyperparameter?\n",
    "\n",
    "I would say that the learning rate is important when it comes to hyperparameter in terms of linear regression model that uses gradient descent. The learning_rate_init initialization parameter lets us specify the value for this important hyperparameter in the SGDRegressor model. We can also change the learning rate and impact during its training process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1053157d",
   "metadata": {
    "papermill": {
     "duration": 0.012958,
     "end_time": "2023-05-16T19:00:16.484108",
     "exception": false,
     "start_time": "2023-05-16T19:00:16.471150",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "In part 2, I first explored the dataset and understand the rows and columns. After I observed what the dataset contains and have it split into traning and testing sets. I dived into applying preprocessing ways (standardization) using the StandardScaler library and input it into a workflow with Pipeline. I tried different hyperparameter configurations, and focused on the learning rate using SGDRegressor. The results doesn't seem to satisfy what I wanted it to do, hence resulted in poor model performance. It was evaluated using both the training and test sets, which were signifcantly negative and poor model choice. I could try to explore nonlinear models, and incorporate more factors that may alter the accuracy. I could apply methods such as transforming features as well. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20.101387,
   "end_time": "2023-05-16T19:00:17.518699",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-16T18:59:57.417312",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
