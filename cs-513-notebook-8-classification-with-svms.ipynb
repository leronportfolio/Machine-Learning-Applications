{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nThis notebook provides you an opportunity to demonstrate proficiency in meeting course learning goals by applying a support vector machine to solve a classification problem using widely-used ML libraries and an ML workflow.\n","metadata":{}},{"cell_type":"markdown","source":"# Mine Detection (Revisited)\n\nIn this notebook, you will revisit [a previously seen classification problem](https://www.kaggle.com/code/bakosy/cs-513-notebook-4-classification-with-perceptrons), and see if you can build a better classification model that can predict whether or not a sonar signature is from a mine or a rock.\n\n<div class=\"alert alert-block alert-warning\">\n<b>Tip:</b> We suggest reviewing your Notebook 4: Classification with Perceptrons.\n</div>\n\nWe'll use a version of the [sonar data set](https://www.openml.org/search?type=data&sort=runs&id=40&status=active) by Gorman and Sejnowski. Take a moment now to [reacquaint yourself with the subject matter of this data set](https://datahub.io/machine-learning/sonar%23resource-sonar), and look at the details of the version of this data set, [Mines vs Rocks, hosted on Kaggle](https://www.kaggle.com/datasets/mattcarter865/mines-vs-rocks).\n\nSimilar to [a previous notebook](https://www.kaggle.com/code/bakosy/cs-513-notebook-4-classification-with-perceptrons), this notebook expects each student to implement the ML workflow steps. We will get you started by providing the first step, loading the data, and providing some landmarks and tips below. Your process should demonstrate:\n\n1. Loading the data\n2. Exploring the data\n3. Preprocessing the data\n4. Preparing the training and test sets\n5. Creating and configuring a sklearn.svm.SVC\n6. Training the SVM\n7. Validating and Testing the SVM\n8. Demonstrating making predictions\n9. Evaluate (and Improve) the results\n\nCan you train a classifier that can predict whether a sonar signature is from a mine or a rock? \"Three trained human subjects were each tested on 100 signals, chosen at random from the set of 208 returns used to create this data set. Their responses ranged between 88% and 97% correct.\" Can your classifier outperform the human subjects?\n\nMost importantly, how does the performance of the SVM classifier compare to the perceptron results observed in [Notebook 4](https://www.kaggle.com/code/bakosy/cs-513-notebook-4-classification-with-perceptrons)?\n\n","metadata":{}},{"cell_type":"markdown","source":"## Step 1: Load the Data\n\nThe first step is to load the Mines vs Rocks dataset into a pandas DataFrame. The dataset is provided as a CSV file without a header row. By loading the CSV file into a DataFrame, we can easily explore the data, preprocess it, and split it into training and test sets.\n\nWe start by importing the pandas library, which provides powerful data manipulation and analysis tools.\nNext, we define the path to the CSV file containing the sonar data. Adjust the file path if needed.\nUsing the `pd.read_csv()` function from pandas, we read the CSV file into a DataFrame called `sonar_data`. We set the header parameter to None to indicate that the CSV file has no header row.\nNow, we have a pandas DataFrame that takes in the sonar data, and we can proceed with our data exploration and analysis.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nsonar_csv_path = \"../input/mines-vs-rocks/sonar.all-data.csv\"\nsonar_data = pd.read_csv(sonar_csv_path, header=None)","metadata":{"execution":{"iopub.status.busy":"2023-05-31T06:00:19.660500Z","iopub.execute_input":"2023-05-31T06:00:19.661359Z","iopub.status.idle":"2023-05-31T06:00:19.679149Z","shell.execute_reply.started":"2023-05-31T06:00:19.661297Z","shell.execute_reply":"2023-05-31T06:00:19.677710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After running this code, we will have the sonar data loaded into the `sonar_data` DataFrame, which we can use for further analysis and preprocessing. ","metadata":{}},{"cell_type":"markdown","source":"## Step 2: Explore the Data\n\nOnce the data has been loaded into the DataFrame, we can start exploring it to understand its structure and characteristics. \n\nFirst, let's check the dimensions of the DataFrame using the shape attribute, which returns the number of rows and columns in the DataFrame. Next, we can inspect the column names using the columns attribute, which returns a list of all the column names in the DataFrame. To get an idea of the data, we will examine a few sample records using the `head()` function, which displays the first few rows of the DataFrame.\n\nLet's begin:\n","metadata":{}},{"cell_type":"code","source":"print(\"Data shape:\", sonar_data.shape)\n\nprint(\"Column names:\", sonar_data.columns)\n\nprint(\"Sample records:\")\nprint(sonar_data.head())","metadata":{"execution":{"iopub.status.busy":"2023-05-31T06:00:19.681701Z","iopub.execute_input":"2023-05-31T06:00:19.682935Z","iopub.status.idle":"2023-05-31T06:00:19.710033Z","shell.execute_reply.started":"2023-05-31T06:00:19.682870Z","shell.execute_reply":"2023-05-31T06:00:19.707263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By running the above code, the given information will be the data shape, column names, and sample records. \n\nData shape: This will display the number of rows and columns in the DataFrame. It provides an overview of the dataset's size.\n\nColumn names: This will show the names of the columns in the DataFrame. Each column represents a feature or attribute of the data.\n\nSample records: The first few records in the DataFrame will be printed, which allows us to observe the actual data values.\n\nAfter running the code, we should be able to see the dimensions of the DataFrame, column names, and a preview of the data. It is important to understand the structure and characteristics of the dataset.","metadata":{}},{"cell_type":"markdown","source":"## Step 3: Preprocess the Data\n\nData preprocessing is an important step that involves handling missing values, encoding categorical variables, and performing feature scaling or normalization if required. It would be helpful to understand the specific preprocessing steps needed for this dataset.\n\nIn the code below, we first separate the features `(X)` and class labels `(y)` from the loaded DataFrame. The features are obtained by selecting all columns except the last one, and the class labels are obtained from the last column.\n\nNext, we use the StandardScaler class from scikit-learn to perform z-score normalization on the features. The `fit_transform()` method of the scaler fits the scaler on the data and then applies the transformation to standardize the feature values.\n\nLastly, we create a new DataFrame `preprocessed_data`, which stores the preprocessed features. This DataFrame will have the same column names as the original data excluding the last column, which contains the class labels. Printing the `preprocessed_data.head()` will show the first few records of the preprocessed data.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nX = sonar_data.iloc[:, :-1] \ny = sonar_data.iloc[:, -1]  \n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\npreprocessed_data = pd.DataFrame(X_scaled, columns=sonar_data.columns[:-1])\nprint(preprocessed_data.head())","metadata":{"execution":{"iopub.status.busy":"2023-05-31T06:00:19.712490Z","iopub.execute_input":"2023-05-31T06:00:19.713043Z","iopub.status.idle":"2023-05-31T06:00:19.736930Z","shell.execute_reply.started":"2023-05-31T06:00:19.712992Z","shell.execute_reply":"2023-05-31T06:00:19.734752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After running the code, we should be able to obtain a DataFrame with the preprocessed features. The feature values will be scaled using z-score normalization. What it does is that it helps ensure that all features have a mean of 0 and a standard deviation of 1. This normalization step can be beneficial for certain machine learning algorithms, including SVMs.","metadata":{}},{"cell_type":"markdown","source":"## Step 4: Prepare the Training and Test Data Sets\n\nIn this step, we will split the preprocessed data into training and test sets using the train_test_split function from scikit-learn. The purpose of this step is to have separate data for training the SVM classifier and evaluating its performance on unseen data. \n\nIn the code below, we use the train_test_split function to split the preprocessed features `(X_scaled)` and the class labels `(y)` into training and test sets. We specify the test size as 0.2, where 20% of the data will be allocated for testing, and the remaining 80% will be used for training. The random_state parameter is set to 42 for reproducibility.\n\nThe function returns four sets: `X_train, X_test, y_train, and y_test`, representing the training and test sets for the features and class labels. After splitting the data, we print the shapes of the training and test sets to verify that the split was successful. The shapes will indicate the number of samples and the number of features in each set.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n\nprint(\"Training set shape:\", X_train.shape, y_train.shape)\nprint(\"Test set shape:\", X_test.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-31T06:00:19.740325Z","iopub.execute_input":"2023-05-31T06:00:19.740831Z","iopub.status.idle":"2023-05-31T06:00:19.754772Z","shell.execute_reply.started":"2023-05-31T06:00:19.740785Z","shell.execute_reply":"2023-05-31T06:00:19.753038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From what we learn, the train_test_split function randomly shuffles the data before splitting it. This randomization helps ensure that the training and test sets are representative of the overall dataset. We see that the training set has a shape of (166, 60), where 166 represents the number of samples and 60 represents the number of features. Similarly, the test set has a shape of (42, 60), indicating 42 samples and 60 features. This means that the confirmation of the split is successful and the classifer will be further trained and evulated based on the correct subsets of the data.","metadata":{}},{"cell_type":"markdown","source":"## Step 5: Instantiate and Configure an SVM\n\n After splitting the data, we can create an instance of the SVM classifier using the SVC class from sklearn.svm module. From reading the documentation, we learn that we can configure various parameters of the SVC, such as the kernel type, regularization parameter, etc.\n \nIn this step, we will instantiate and configure an SVM classifier using the SVC class from scikit-learn. The SVM classifier is for solving classification problems and can be configured with various parameters to optimize its performance based on the code chunk below.","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvm_classifier = SVC(kernel='rbf', C=1.0, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-05-31T06:00:19.756253Z","iopub.execute_input":"2023-05-31T06:00:19.756813Z","iopub.status.idle":"2023-05-31T06:00:19.774084Z","shell.execute_reply.started":"2023-05-31T06:00:19.756744Z","shell.execute_reply":"2023-05-31T06:00:19.772716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After creating the classifier, we print the default configuration of the SVM classifier to get an overview of its settings. This can help us understand the default values of various parameters and their effects on the classifier.\n\nBased on the documentation of the SVC class, we are able to configure the SVM classifier by adjusting specific parameters using the `set_params` method as well.\n\nWe then fit the SVM classifier to the training data using the fit method. This step involves learning the optimal decision boundaries from the labeled training examples.","metadata":{}},{"cell_type":"code","source":"print(\"Default SVM classifier configuration:\")\nprint(svm_classifier)\n\nsvm_classifier.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-05-31T06:00:19.775360Z","iopub.execute_input":"2023-05-31T06:00:19.775686Z","iopub.status.idle":"2023-05-31T06:00:19.796167Z","shell.execute_reply.started":"2023-05-31T06:00:19.775658Z","shell.execute_reply":"2023-05-31T06:00:19.794060Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 6: Train the SVM\n\nWe first fit the SVM classifier to the training data using the `fit()` method. This step involves learning the optimal decision boundaries from the labeled training examples that we gave.\n \nIn this step, we will train the SVM classifier by fitting it to the training data. The SVM will learn the optimal decision boundaries from the labeled training examples. For the code below, the `fit()` method of the SVM classifier is used to train the model. The `X_train` contains the preprocessed feature data, and `y_train` contains the corresponding class labels.\n\n","metadata":{}},{"cell_type":"code","source":"svm_classifier.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-05-31T06:00:19.798009Z","iopub.execute_input":"2023-05-31T06:00:19.798346Z","iopub.status.idle":"2023-05-31T06:00:19.811470Z","shell.execute_reply.started":"2023-05-31T06:00:19.798309Z","shell.execute_reply":"2023-05-31T06:00:19.810180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"During the training process, the SVM classifier will analyze the training data and learn patterns that distinguish mines from rocks. The random_state=42 parameter is  used to set the random seed for reproducibility in ML algorithm. \n\nAfter running the code, the SVM classifier will be trained, and the learned model will be stored in svm_classifier. It will contain the optimized decision boundaries that can be used to make predictions on unseen data. The SVM algorithm performs an iterative optimization process to find the best decision possible, which may require multiple iterations over the training data.","metadata":{}},{"cell_type":"markdown","source":"## Step 7: Validate and Test the SVM\n\nAfter training the SVM classifier, we need to evaluate its performance on both the training and test sets. This will help us assess how well the classifier has learned the patterns in the training data and how well it can generalize to unseen data.\n\nWe use the `score()` method of the SVM classifier to calculate the accuracy of the classifier on the training set. The `score()` method takes the preprocessed feature data for the training set `(X_train)` and the corresponding class labels `(y_train)` as inputs. The returned value represents the accuracy of the classifier on the training set.\n\nIn the provided code, the accuracy of the classifier on the training set is stored in the training_accuracy variable.\n\nFinally, we print the training accuracy using the `print()` function.","metadata":{}},{"cell_type":"code","source":"training_accuracy = svm_classifier.score(X_train, y_train)\nprint(\"Training Accuracy:\", training_accuracy)\n\ntest_accuracy = svm_classifier.score(X_test, y_test)\nprint(\"Test Accuracy:\", test_accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-05-31T06:00:19.812614Z","iopub.execute_input":"2023-05-31T06:00:19.812884Z","iopub.status.idle":"2023-05-31T06:00:19.829944Z","shell.execute_reply.started":"2023-05-31T06:00:19.812858Z","shell.execute_reply":"2023-05-31T06:00:19.828768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After running the code above, we obtain the training accuracy of the SVM classifier on the training set accuracy of approximately 98.2%.The training accuracy represents the proportion of correctly predicted labels in the training set. It also indicates how well the SVM classifier fits the training data and captures the underlying patterns based on my understanding.\n\nNext, we'll do the same for the test accuracy below. Run the code to see the test accuracy.","metadata":{}},{"cell_type":"code","source":"test_accuracy = svm_classifier.score(X_test, y_test)\nprint(\"Test Accuracy:\", test_accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-05-31T06:00:19.833576Z","iopub.execute_input":"2023-05-31T06:00:19.834089Z","iopub.status.idle":"2023-05-31T06:00:19.844070Z","shell.execute_reply.started":"2023-05-31T06:00:19.834046Z","shell.execute_reply":"2023-05-31T06:00:19.842036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After running this code, we will obtain the training accuracy and the test accuracy of the SVM classifier. The training accuracy indicates how well the classifier has learned from the training data, while the test accuracy represents the performance of the classifier on unseen data. In the provided code snippet, the test accuracy is approximately 90.5%, indicating that the SVM classifier performs well on the test set.\n\n","metadata":{}},{"cell_type":"markdown","source":"## Step 8: Demonstrate Making Predictions\n\nIn this step, we will use the trained SVM classifier to make predictions on new, unseen data samples. This will help us understand how the classifier performs on data that it hasn't seen during the training process.\n\nWe select a subset of samples from the training set, `X_train`, to represent the new, unseen data samples. In the provided code snippet, we select the first 5 samples using `X_train[:5]`. Then we use the `predict()` method of the SVM classifier, svm_classifier, to predict the class labels for the given features. The `predict()` method takes the features as input and returns the predicted class labels.\n\nFinally, we print the predicted class labels using a loop to show the classification output for each specific sample.","metadata":{}},{"cell_type":"code","source":"sample_features = X_train[:5]\n\npredictions = svm_classifier.predict(sample_features)\n\nprint(\"Predicted labels:\")\nfor prediction in predictions:\n    print(prediction)","metadata":{"execution":{"iopub.status.busy":"2023-05-31T06:00:19.846715Z","iopub.execute_input":"2023-05-31T06:00:19.847257Z","iopub.status.idle":"2023-05-31T06:00:19.859246Z","shell.execute_reply.started":"2023-05-31T06:00:19.847206Z","shell.execute_reply":"2023-05-31T06:00:19.857472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After running this code, we will obtain the predicted class labels for the new, unseen data samples. Each predicted label represents the classification output for a specific sample. We can assess how well the SVM classifier performs on these samples and compare the predictions with the true class labels to evaluate the model's accuracy. ","metadata":{}},{"cell_type":"markdown","source":"## Step 9: Evaluate (and Improve?)\n\nThe SVM classifier was configured with the following parameters: `kernel='rbf', C=1.0, and random_state=42`. It achieved a training accuracy of approximately 98.2% and a test accuracy of around 90.5%.\n\nThe classifier performs well overall, with high accuracy on both the training and test sets. However, it's worth noting that the accuracy on the training set is higher than the accuracy on the test set, indicating some level of overfitting. This means that the classifier may have memorized the training data too well and is not generalizing as effectively to new, unseen data.\n\nCompared to the perceptron classifier from Notebook 4, the SVM classifier shows better performance. The perceptron achieved an accuracy of 79%, while the SVM classifier achieved around 90.5% on the test set. Basically, the SVM classifier outperforms the perceptron in terms of accuracy for this specific problem.\n\n## Conclusion\nWe loaded the Mines vs Rocks dataset and performed the following steps required to fulfill this notebook:\n\nWe first explored the data by examining its dimensions, column names, and sample records. Next, we preprocessed the data by separating the features and class labels, and then applying z-score normalization using StandardScaler.\nThen we prepared the training and test sets using the `train_test_split function`.\nInstantiated and configured an SVM classifier using the SVC class from scikit-learn. After that, we trained the SVM classifier by fitting it to the training data. We also validated and tested the SVM classifier's performance on the training and test sets. Next, we had to demonstrated making predictions on new, unseen data using the trained SVM classifier.\n\nOne way to improve the SVM classifier is that we can experiment with hyperparameter tuning, feature selection/engineering, and ensemble methods. These approaches could potentially enhance the classifier's performance and increase accuracy.\n\nFrom what I've read, I believe the SVM classifier could potentially be improved further. \n\n#### Four things that we can incorporate are: \n\nHyperparameter Tuning: We can play around or experimenting with different values for the hyperparameters of the SVM classifier.\n\nFeature Selection or Engineering: We can analyzing the dataset and selecting or engineering informative features that have a stronger correlation with the target variable.\n\nEnsemble Methods: When exploring ensemble methods, such as combining multiple SVM classifiers with different hyperparameters or using other classifiers in conjunction with the SVM classifier, we can create a more robust and accurate model.\n\nFuture work could involve exploring additional preprocessing techniques, trying different classification algorithms, or gathering more data to further improve the classifier's performance. The SVM classifier shows great results, but there is always room for improvement. \n","metadata":{}}]}